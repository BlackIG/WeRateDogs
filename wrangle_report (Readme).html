Reporting: wrangle_report
Introduction:
This html report briefly describes my wrangling efforts on this project from gathering to assessing to cleaning the datasets in 300 - 600 words. Worthy of note is the fact that this project completed outside Udacity's workspace.

Overview:
The dataset I wrangled belongs to Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog.

Gathering:
I gathered three pieces of datasets through different means. Below are the three datasets:

The WeRateDogs Twitter archive: this was a pre-provided flat file, specifically, a CSV file with the name "twitter-archive-enhanced.csv". All I had to do was click the link given and download manually. Then I read the file from my working directory into a pandas DataFrame using the pd.read_csv() function. This dataset contained info about the tweet and the dog rating
The text of the link was twitter_archive_enhanced.csv but the name of the file is "twitter-archive-enhanced.csv". It is important to note that the file should be read with the file name itself and not the text of the link

The tweet image predictions: This TSV file was downloaded programmatically from URL on Udacity's servers, using the Requests library and saved in a folder called image-predictions. This dataset contained the url to the images of the dogs and the predictions of the breed.

Retweet & favorite count: Additional data gotten from querying Twitter's API with Tweepy library, and saved in a file called in a file called tweet_json.txt file.

Assessing:
On the datasets, I carried out visual assessment by looking at the display in Jupyter notebook and programmatic assessment by using pandas' functions and/or methods.

My key deliverables here was to deetect and document at least eight (8) quality issues and two (2) tidiness issues, and I did

Cleaning:
To clean all of the issues you documented while assessing, I made a copy of the datasets to avoid loss of data. Then I employed define-code-test framework for documenting my cleaning efforts. A large chunk of the time on this project was spent cleaning the data.

After cleaning the three datasets, I merged themm into one master dataset according to the rules of tidy data.

Storing:
The master dataset was stored as a CSV file in my working directory
